# Модель классификации комментариев

Прогресс не стоит на месте. Применяем машинное обучение для ... **текстов**.

Цель проекта - подготовить модель, способную определить токсичный комментарий, со значением метрики качества *f1* не меньше **0.75**.

**В ходе проекта:**

- загрузили данные, ознакомились с ними, увидели дисбаланс классов (только 10% токсичных комментариев от общего количества)
- выполнили лемматизацию и очистку текста с помощью средств библиотеки `nltk` и специально созданных функций
- рассчитали  $(\mathbf{TFIDF})$, , то есть перевели текст на понятный машине язык, привели его к векторному виду
- проанализировали *ROC*-кривую для модели логистической регрессии, но по ней не удалось определить порог, при котором целевая метрика может быть достигнута
- изменили порог классификации вручную, что позволило улучшить *f1*-меру до **целевого значения** для модели логистической регрессии
- взвешивание классов для моделей логистической регрессии и решающего дерева не привело к существенному улучшению целевой метрики

Наилучшее решение из рассморенных - *подбор порога классификации вручную для модели логистической регрессии*, достигнуто значение *f1*-меры равное **0.78**.

___

## Библиотеки и инструменты

`pandas`, `numpy`, `seaborn`, `matplotlib`, `re`, `nltk`, `sklearn`

`TfidfVectorizer`, `stopwords`, `LogisticRegression` , `DecisionTreeClassifier`, `RandomForestClassifier`, `DummyClassifier`,  `train_test_split`, `pyplot`, `accuracy_score`, `confusion_matrix`, `classification_report`, `f1_score`, `roc_curve`, `roc_auc_score`, `WordNetLemmatizer`

## Описание данных

В нашем распоряжении около **160 тысяч текстовых комментариев**
Данные находятся в файле `toxic_comments.csv`. 
Столбец `text` в нём содержит текст комментария, а `toxic` — целевой признак.

